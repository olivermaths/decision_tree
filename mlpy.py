# -*- coding: utf-8 -*-
"""mlpy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KBHcwtdkbn7oZaj2KkW2EDAQQMrPZMm7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy.random import choice, randint

"""Firebase settings:"""

import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

"""**Meus dados:**"""

training_data = [
    ['Green', 3, 'Apple'],
    ['Yellow', 3, 'Apple'],
    ['Red', 1, 'Grape'],
    ['Red', 1, 'Grape'],
    ['Red', 3, 'Strawberry'],
    ['Yellow', 3, 'Lemon'],
]

"""**Esse header é só caso eu vá montar uma planilha**"""

header = ["Color", "Diameter", "Label"]



"""**Funções uteis:**"""

def unique_value(rows, col):
    return set([row[col] for row in rows])

unique_value(training_data, 0)

def class_counts(rows):
  counts = {}
  for row in rows:
    label = row[-1]
    if label not in counts:
      counts[label] = 0
    counts[label] += 1
  return counts

class_counts(training_data)

def is_numeric(value):
  return isinstance(value, int) or isinstance(value, float)

is_numeric(7)

"""**Um class para criar perguntas nos "nó" da arvore**"""

class Question:
  def __init__(self, column, value):
    self.column = column
    self.value = value
  def match(self, example):
    val = example[self.column]
    if is_numeric(val):
      return val >= self.value
    else:
      return val == self.value
  def __repr__(self):
    condition = "=="
    if is_numeric(self.value):
      condition = ">="
    return "Is %s %s %s?" % (header[self.column], condition, str(self.value))

Question(0,'Red')

q = Question(1, 3)
q

# Dividir os dados com base na pergunta
def partition(rows, question):
  true_rows, false_rows = [], []
  for row in rows:
      if question.match(row):
        true_rows.append(row)
      else:
        false_rows.append(row)
  return true_rows, false_rows

true_rows, false_rows = partition(training_data, Question(1, 3))
true_rows

# Essa é a função principal, eu usei a formula mais simples para calcular a impuridade da divisão dos dados com base na pergunta
def gini(rows):
  counts = class_counts(rows)
  impurity = 1
  for lbl in counts:
      prob_of_lbl = counts[lbl]/float(len(rows))
      impurity -= prob_of_lbl**2
  return impurity

gini(true_rows)

no_mixing = [['Apple'],['Apple']]
gini(no_mixing)

gini(training_data)

# Função  para calcular se a divisão da arvore resultou em ganho de certeza
def info_gain(left, right, current_uncertainty):
    p = float(len(left)/(len(left) + len(right)))
    return current_uncertainty - (p * gini(left)) - ((1 - p) * gini(right))

# Função para escolher a melhor pergunta com base na certeza ganha pela pergunta
def find_best_split(rows):
    best_gain = 0
    best_question = None
    current_uncertainty = gini(rows)
    n_features = len(rows[0]) - 1
    for col in range(n_features):
        values = set([row[col] for row in rows])
        for val in values:
          question = Question(col, val)
          true_rows, false_rows = partition(rows, question)
          if len(true_rows) == 0 or len(false_rows) == 0:
            continue
          gain = info_gain(true_rows, false_rows, current_uncertainty)
          if gain >= best_gain:
              best_gain, best_question = gain, question
    return best_gain, best_question

best_gain, best_question = find_best_split(training_data)
best_question

# Os "nós" da arvore
class Decision_Node:
    def __init__(self, question, gain, true_branch, false_branch):
      self.question = question
      self.gain = gain
      self.true_branch = true_branch
      self.false_branch = false_branch

class Leaf:
    def __init__(self, rows):
        self.predictions = class_counts(rows)

def build_tree(rows):
  gain, question = find_best_split(rows)
  if gain == 0:
    return Leaf(rows)
  true_rows, false_rows = partition(rows, question)
  true_branch = build_tree(true_rows)
  false_branch = build_tree(false_rows)
  return Decision_Node(question,gain, true_branch, false_branch)

def print_tree(node, spacing=" "):
    if isinstance(node, Leaf):
        print (spacing + "Predict", node.predictions)
        return
    print (spacing + str(node.question))
    print (spacing + "gain:" + str(node.gain))
    print (spacing + '--> True:')
    print_tree(node.true_branch, spacing + "  ")
    print (spacing + '--> False:')
    print_tree(node.false_branch, spacing + "  ")

my_tree = build_tree(training_data)
print_tree(my_tree)

def classify(data, tree):
    if isinstance(tree, Leaf):
        return tree.predictions
    if tree.question.match(data):
        return classify(data, tree.true_branch)
    else:
        return classify(data, tree.false_branch)

classify(['Green', 3, 'Apple'], my_tree)

def print_leaf(counts):
    total = sum(counts.values())*1.0
    probs = {}
    for lbl in counts.keys():
        probs[lbl] = str(int(counts[lbl] / total * 100)) + "%"
    return probs

print_leaf(classify(['Red', 3, 'Strawberry'], my_tree))

print_leaf(classify(['Red', 2, 'Strawberry'], my_tree)) # uma formula de gain melhor evitaria isso, creio eu. Dentre outras coisas.

print_leaf(classify(['Yellow', 3, 'Lemon'], my_tree))